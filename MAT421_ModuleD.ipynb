{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAT421_ModuleD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOc/VcFtDj4D+Meqe0vwkhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaleftwi/MAT421_Modules/blob/main/MAT421_ModuleD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module D** *Linear Equations and Related Topics*\n",
        "\n",
        "---\n",
        "The next section will cover some topics regarding linear algebra. Topics covered in Module D will include the following:\n",
        "\n",
        "*   1.1 Introduction\n",
        "*   1.2 Elements to Linear Algebra\n",
        "*   1.3 Linear Regression\n",
        "\n",
        "\n",
        "For the sake of clarity, some of these sections will be broken up into smaller components for further elaboration."
      ],
      "metadata": {
        "id": "4I31frPSKYcg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1.1 Introduction**\n",
        "---\n",
        "It is not practical to deliver the entirety of linear algebra within the confines of a single module. However, this will be an opportunity to touch upon surface-level concepts and to relate some of its principals to computational analysis.\n",
        "\n",
        "As a whole, linear algebra observes matrices and vectors- diving into topics such as orthogonality, matrix decomposition, vector spaces, and eigenvalues. These concepts are often used and applied in fields such as data science and linear regression.\n",
        "\n",
        "It cannot be understated how important of a role linear algebra plays in many various data science and machine learning problems."
      ],
      "metadata": {
        "id": "LCllfRIXKx69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1.2 Elements to Linear Algebra**\n",
        "---\n",
        "To better understand this section, the following topics will be examined separately:\n",
        "\n",
        "*   Linear Spaces\n",
        "*   Orthogonality\n",
        "*   Gran-Schmidt Process\n",
        "*   Eigenvalues and Eigenvectors\n",
        "\n"
      ],
      "metadata": {
        "id": "fxRjf8yLKYTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Linear Spaces\n",
        "\n",
        "---\n",
        "At the heart of linear algebra is the vector. Vectors do not have a fixed length. For this page, vectors will be written in the form v = {[a, b, c ... ]}. Various vectors of diverse forms can be added together to make linear combinations of vectors. That is to say the linear combination vc = a*v1 + b*v2, in which v1 and v2 are vectors and a and b are constants. As more vectors are included, a larger range of sums can be produced by these combinations. This diverse sums are called the span of the linear combinations- otherwise known as the span.\n",
        "\n",
        "When a new vector is compared to a group of vectors, the vector is considered linear independent when there does not exist a linear combination of the pre-existing vectors that can form the questioned vector. The dimension of a linear space is the number of independent vectors that span the created space.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qynolE4TvyIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Orthogonality\n",
        "\n",
        "---\n",
        "Two linearly independent vectors are also known to be orthogonal. However, this alone may not be sufficient is solving high class linear algebra problems. Many times orthogonal vectors must be converted to orthonormal vectors. To do this, the norm of the vectors must be 1, and the orthgonal vectors must be multiplied by a constant to make this change.\n",
        "\n",
        "One of the applications of orthonormality is the best approximation theorem. In this case we have a vector v and a approximation v\\*. The goal is to minimize the distance between v and v\\* by choosing the best approximation. This approximation v* is formed from a vector multiplication of a vector v and a unit vector orthonormal to v."
      ],
      "metadata": {
        "id": "Pb-gbDeOv_US"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Gran-Schmidt Process\n",
        "\n",
        "---\n",
        "The Gran-Schmidt Process is used to find the orthonormal bases. According to a theorem, a span of vectors span{v1, v2, ... , vn} have an associated orthonormal basis {q1, q2, ... , qn}. \n",
        "\n",
        "This is qi = bi / ||bi|| in which bi is ai - P*ai\n",
        "\n",
        "Here P is the projection of the vector."
      ],
      "metadata": {
        "id": "TFEce3kZv_Jn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Eigenvalues and Eigenvectors\n",
        "\n",
        "---\n",
        "Eigenvalues and eigenvectors are key deconstructors of matrices, regardless of the problem. Lambda is an eigenvalue of matrix A if there exists a vector x such that:\n",
        "\n",
        "Ax = (Lambda)x\n",
        "\n",
        "Here the corresponding vector x is known as the eigenvector.\n",
        "\n",
        "It should be noted that a matrix can have various distinct eigenvalues (up to its dimension) and in other cases a matrix may have repeated or even no eigenvalues."
      ],
      "metadata": {
        "id": "-20eZ76av-6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1.3 Linear Regression**\n",
        "---\n",
        "To better understand this section, the following topics will be examined separately:\n",
        "\n",
        "*   QR Decomposition\n",
        "*   Least Squares Problem\n",
        "*   Linear Regression"
      ],
      "metadata": {
        "id": "Do3YO8eeMhAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### QR Decomposition\n",
        "\n",
        "---\n",
        "One of the most useful procedures to solve least squared problems is known as QR decomposition.\n",
        "\n",
        "The first step is to obtain a span of orthonormal vectors derived from a set of vectors. This can be done by the Gran-Schmidt Process. Once the orthonormal basis span(a1, a2, ... , an) is observed, to deconstruct matrix A let A = [a1, a2, ... , an] together as a single matrix. The output of the QR decomposition will follow in the form of A = QR. Q is a matrix that satisfies Q(transpose)\\*Q = I. On the otherhand, R is a upper triangular matrix (0's below the diagonal), in which R is a linear combination of Q's to reform A.\n",
        "\n",
        "Thus it is also true that:\n",
        "\n",
        "A(transpose) = R(transpose) \\* Q(transpose)"
      ],
      "metadata": {
        "id": "gn2UuirdwTBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Least Squares Problem\n",
        "\n",
        "---\n",
        "In the least squares problem, matrix A is given along with vector b. The goal of the problem is to approximate b by the matrix product Ax (with x being an approximated vector). Thus it is important to find the minimum of ||Ax-b|| to solve the least squared problem.\n",
        "\n",
        "One common solution is to use the transpose of A to solve for x:\n",
        "\n",
        "A(transpose)\\*A\\*x = A(transpose)\\*b\n",
        "\n",
        "Of course such multiplication can occur only if A(transpose) has the same matrix dimensions as A.\n",
        "\n",
        "Thus one soltution builds on this formula by deconstructing A(transpose)A into a different matrix altogether. Or to be more specific, QR decomposition is used on the matrix to change the solution into the following:\n",
        "\n",
        "Q\\*R\\*x = Q\\*Q(transpose)\\*b\n",
        "\n",
        "When simplified, it is found that the solution to:\n",
        "\n",
        "R\\*x = Q(transpose)\\*b\n",
        "\n",
        "solves the least squares problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "kkpiOxRNwSrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Linear Regression\n",
        "\n",
        "---\n",
        "If we are given a set of data point {(x, y)}. A common sentiment and approach to make sense of the data is to create a best fit line. \n",
        "\n",
        "In which estimateY = beta0 + beta1\\*x1 + beta2\\*x2 + ... + betan\\*xn\n",
        "\n",
        "The problems can be understood best as the minimum of ||y-A*beta||^2. This is restating the least squares problem, and therefore can be solved in the same way."
      ],
      "metadata": {
        "id": "7K-oOPo1wSBR"
      }
    }
  ]
}