{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAT421_ModuleE1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyODf41v98HRou0+yFwgjiZx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaleftwi/MAT421_Modules/blob/main/MAT421_ModuleE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Module E (Part 1)** *Differentiation and Taylor Approximation*\n",
        "\n",
        "---\n",
        "The next section will begin a surface level dive into some calculus concepts. This will be the first of two breakdowns of ideas and theories covered in the MAT 421 course.\n",
        "\n",
        "*   3.2 Continunity and Differentiation\n",
        "*   3.3 Unconstrained Optimization\n",
        "\n",
        "Similar to previous workbooks, this module will break apart these sections into smaller components for further clarification of the discussed material. Of times, many optimization problems can be broken down by calculus."
      ],
      "metadata": {
        "id": "S6R6zwIDQjHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3.2 Continuity and Differentiation**\n",
        "---\n",
        "To better understand this section, the following topics will be examined separately:\n",
        "\n",
        "*   Limits and Continuity\n",
        "*   Derivatives\n",
        "*   Taylor's Theorem"
      ],
      "metadata": {
        "id": "L1thpuEySVbT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Limits and Continuity\n",
        "\n",
        "---\n",
        "It is vital to understand the notion of limits in order to better understand concepts such as continuity, derivatives and integrals. Simply put, limit points (also known as limits or accumulation points) exist in a set S if and only if there exists a point \"x\" such that there is an element \"a\" of S in which ||x - a|| < r, for every value \"r\" > 0.\n",
        "\n",
        "It should be noted that \"x\" may not necessarily exist in set S, but as the value of \"r\" approaches 0, the value \"a\" approaches the value \"x\". It is this convergent value that is deemed the limit L.\n",
        "\n",
        "In relation to continuity, a function is deemed continuous if for all value in the function's domain, the limit of the function is equvalent to the value evaluated at that point in the function.\n",
        "\n",
        "Lim(f(x)) (as x approaches a) = f(a).\n",
        "\n"
      ],
      "metadata": {
        "id": "jGgd4RABX41b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Derivatives\n",
        "---\n",
        "The derivative of a function measures the change of value in a function relative to its intial position.\n",
        "\n",
        "f'(x0) = df(x0)/dx = Lim(h approaches 0) ((fx0 + h) - f(x0)) / h\n",
        "\n",
        "Some may classify derivatives as the rate of change or slope at a given point.\n",
        "\n",
        "One of the many theorem that handle derivatives is the Mean Value Theorem. This helpful theorem indicates that on an interval [a,b] for which a continuous function is defined (in which as derivatives exists) it holds that there exists c ∈ [a,b], in which (f(b) - f(a))/(b - a) = f'(c). The interpretation of this is that the average slope from a to b will exists as a derivative between the two points at a third point \"c\".\n",
        "\n",
        "Derivatives are applicable in more than just single value cases as well. Oftimes in various fields, multiple independent variable are used in determining an outcome. To account for each variable, we take a partial derivative in respect to one variable at a time:\n",
        "\n",
        "∂f(x0)/∂xi = Lim (h approaches 0) (((f(x0) + h*ei) - f(x0)) / h)\n",
        "\n",
        "Then out of these partial derivatives is form the Jacobian Matrix or otherwise known as the gradient ∇f(x0). This matrix uses \"d\" independent variables as \"m\" derivatives.\n",
        "\n",
        "Jf(x0) = [∂f1(x0)/∂x1, ... ,∂f1(x0)/∂xd; ... ; ∂fm(x0)/∂x1, ... ∂fm(x0)/∂xd]"
      ],
      "metadata": {
        "id": "-eIYuzTtX4iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Taylor's Theorem\n",
        "\n",
        "---\n",
        "For some functions, evaluating derivatives around a point may become cumbersome. Thus, to approximate a differentiable function around a given point oftimes Taylor's Theorem is handily employed.\n",
        "\n",
        "Taylor's Theorem is actually a generalization of the Mean Value Theorem. Essentially, given function on domain [a,b] that has \"m\" continuous derivatives, Taylor's Theorem may weave together a polynomial that may be used to approximate the function at a given point:\n",
        "\n",
        "f(b) = f(a) + (b-a)(f'(a)) + (1/2)(b-a)^2(f''(a)) + ... + (1/(m-1)!)(b-a)^(m-1)(f^(m-1)(a)) + Rm\n",
        "\n",
        "The last term Rm, is evaluated as (b-a)^m / (m!) * f^(m)(a) * (a + Θ(b-a)) for some Θ in (0,1).\n",
        "\n",
        "This is true not only for single value, but also multivariate cases. In which case, the approximation is adjusted for matrices:\n",
        "\n",
        "f(x) = f(x0) + ∇f(x0 + ξp)^T * p\n",
        "\n",
        "Here ξ ∈ (0,1) and p = x-x0.\n"
      ],
      "metadata": {
        "id": "wk42jUswSu5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 3.3 Unconstrained Optimization**\n",
        "---\n",
        "To better understand this section, the following topics will be examined separately:\n",
        "\n",
        "*   Necessary and Sufficent Conditions of Local Minimizers\n",
        "*   Convexity and Global Minimizers\n",
        "*   Gradient Descent"
      ],
      "metadata": {
        "id": "mKafiEVpSNgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Necessary and Sufficent Conditions of Local Minimizers\n",
        "\n",
        "---\n",
        "Firstly, it is important to distingish the difference between local and global minimizers. A global minimizers x\\* exist in the domain if for all value in the domain, f(x) >= f(x\\*). Of course this requires testing of all value within the domain- which may prove cumbersome. Thus, oftimes we may instead find the local minimizer x\\* in which f(x) >= f(x\\*). This time however, x\\* is a value within the domain that is only compared with the domain values nearby.\n",
        "\n",
        "There may be multiple local minimizers within the function, but there can only exist a single global minimum (and the global minimizers are only the \"x\" values associated with that minimum).\n",
        "\n",
        "One way to understand minimums is the concept of descent direction. A vector v indicates a descent direction for the function if f(x0 + v\\*c) < f(x0), for a c>0."
      ],
      "metadata": {
        "id": "bThII1lnT0Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Convexity and Global Minimizers\n",
        "\n",
        "---\n",
        "A real valued function is convex if a line segment that can be formed between two values on the function is greater than the actual values between the those two points. By considering a function convexity, we can determine local minimizers in the context of global minimizers.\n",
        "\n",
        "Generally speaking, a function is convex is for all x,y in the domain:\n",
        "\n",
        "f((1-α)\\*x+α\\*y) <= (1-α)\\*f(x) + α\\*f(y) given that α ∈ [0,1]\n",
        "\n",
        "This may seem fairly complicated, but this equation is to ensure that all points within the domain can form a line that satisfies having values in the function \"below\" the formed line.\n",
        "\n",
        "Given a convex function f, the point x\\* that exists in which ∇f(x\\*)=0 indicates that x\\* is a global minimizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "0svvFIOqT0EY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### Gradient Descent\n",
        "\n",
        "---\n",
        "Gradient descent is the process that is used to determine a local minimum if a differentiable function. Initially it may seem most convenient to simply find the smallest stationary point produced by the function (stationary points being when ∇f(x) = 0). However, this approach only is efficient with convex functions. Thusly, alternate approaches are used to find minimizers.\n",
        "\n",
        "For example, the steepest descent approach find the smallest values of the function by iteratively determining the direction in which the function f decreases. The goal is to find v\\* that satisfies:\n",
        "\n",
        "∂f(x0)/∂v >= ∂f(x0)/∂v\\* where v\\* = -(∇f(x0) / ||∇f(x0)||)\n",
        "\n",
        "At each determination of the direction of the descent, we iterativly take a step and continue to evaluate the next direction for descent."
      ],
      "metadata": {
        "id": "wdndaYmqTz36"
      }
    }
  ]
}